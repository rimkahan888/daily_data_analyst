Absolutely! Iâ€™ll walk you through generating **synthetic sales data** for your sports apparel chain (5 stores, 50 products, 2 years of daily sales) using Python. This dataset will allow you to perform **ABC analysis** for customers (high/low spenders) and inventory (fast/slow-moving items). Letâ€™s get started:

---

### **Step 1: Setup & Dependencies**
```python
import pandas as pd
import numpy as np
from faker import Faker
from datetime import datetime, timedelta

# Initialize tools
fake = Faker()
np.random.seed(42)  # For reproducibility
```

---

### **Step 2: Generate Dimension Tables**
#### **a. Products (50 SKUs)**
```python
# Generate 50 products with categories and prices
categories = ['Shorts', 'T-Shirts', 'Sneakers', 'Swimwear', 'Accessories']
brands = ['Nike', 'Adidas', 'Under Armour', 'Puma', 'New Balance']

products = pd.DataFrame({
    'product_id': range(1, 51),
    'product_name': [f"{brand} {category} {i}" for i, (brand, category) in 
                     enumerate(zip(np.random.choice(brands, 50), np.random.choice(categories, 50)))],
    'category': np.random.choice(categories, 50),
    'brand': np.random.choice(brands, 50),
    'price': np.round(np.random.uniform(20, 150, 50), 2)  # Prices between $20-$150
})
```

#### **b. Stores (5 Florida Stores)**
```python
stores = pd.DataFrame({
    'store_id': range(1, 6),
    'store_name': ['Miami Beach', 'Orlando Downtown', 'Tampa Bay', 'Jacksonville', 'Fort Lauderdale'],
    'city': ['Miami', 'Orlando', 'Tampa', 'Jacksonville', 'Fort Lauderdale']
})
```

#### **c. Customers (Mix of Loyal & One-Time Buyers)**
```python
# 1000 customers (20% "loyal" with high CLV)
customers = pd.DataFrame({
    'customer_id': range(1, 1001),
    'customer_name': [fake.name() for _ in range(1000)],
    'loyalty_tier': np.random.choice(['Bronze', 'Silver', 'Gold'], size=1000, p=[0.7, 0.2, 0.1])
})
```

#### **d. Time (2 Years of Dates)**
```python
dates = pd.date_range(start='2022-01-01', end='2023-12-31', freq='D')
time_dim = pd.DataFrame({
    'time_id': [int(date.strftime('%Y%m%d')) for date in dates],
    'date': dates,
    'month': dates.month_name(),
    'year': dates.year,
    'is_holiday': np.where(dates.month.isin([11, 12]), True, False)  # Holidays in Nov/Dec
})
```

---

### **Step 3: Generate Fact Sales Data**
#### **Key Patterns to Simulate**:
- **Seasonality**: Higher sales in Q4 (holidays) and summer (swimwear in Florida).
- **Customer Behavior**: 
  - 20% of customers (Gold/Silver) make 80% of purchases.
  - 50% of customers are one-time buyers.
- **Product Demand**: 
  - Top 10 SKUs generate 50% of revenue.
  - Bottom 20 SKUs are slow-moving (clearance items).

```python
# Generate 2 years of transactions (~54k rows)
transactions = []
current_transaction_id = 1

for date in dates:
    for store_id in stores['store_id']:
        # Simulate 30 daily transactions/store (Poisson-distributed for variability)
        daily_transactions = np.random.poisson(lam=30)
        
        for _ in range(daily_transactions):
            # Assign customer (20% chance of repeat buyer)
            if np.random.rand() < 0.2:
                customer_id = np.random.choice(customers[customers['loyalty_tier'] != 'Bronze']['customer_id'])
            else:
                customer_id = np.random.choice(customers['customer_id'])
            
            # Randomly select 1-5 products per transaction
            num_items = np.random.randint(1, 6)
            selected_products = products.sample(num_items, weights=np.linspace(0.1, 1, 50))  # Top SKUs favored
            
            for _, product in selected_products.iterrows():
                transactions.append({
                    'transaction_id': current_transaction_id,
                    'time_id': int(date.strftime('%Y%m%d')),
                    'store_id': store_id,
                    'customer_id': customer_id,
                    'product_id': product['product_id'],
                    'units_sold': 1,
                    'revenue': product['price']
                })
            
            current_transaction_id += 1

fact_sales = pd.DataFrame(transactions)
```

---

### **Step 4: Add Realistic Noise**
```python
# Simulate holiday sales spikes (20% increase in Nov/Dec)
holiday_mask = fact_sales['time_id'].astype(str).str.slice(4, 6).isin(['11', '12'])
fact_sales.loc[holiday_mask, 'revenue'] = fact_sales.loc[holiday_mask, 'revenue'] * 1.2

# Add returns (5% of transactions)
return_mask = np.random.choice([0, 1], size=len(fact_sales), p=[0.95, 0.05])
fact_sales['units_sold'] = np.where(return_mask, -1, 1)
fact_sales['revenue'] = fact_sales['revenue'] * fact_sales['units_sold']
```

---

### **Step 5: ABC Analysis Examples**
#### **a. Customer ABC Analysis (Top 20%)**
```python
# Calculate total spend per customer
customer_spend = fact_sales.groupby('customer_id')['revenue'].sum().reset_index()
customer_spend['CLV_rank'] = customer_spend['revenue'].rank(pct=True)

# Classify ABC
customer_spend['ABC'] = pd.cut(
    customer_spend['CLV_rank'],
    bins=[0, 0.2, 0.5, 1],
    labels=['A (Top 20%)', 'B (Next 30%)', 'C (Bottom 50%)']
)

print(customer_spend.head())
```

#### **b. Inventory ABC Analysis (Top SKUs)**
```python
# Calculate revenue per SKU
product_performance = fact_sales.groupby('product_id')['revenue'].sum().reset_index()
product_performance['revenue_pct'] = product_performance['revenue'] / product_performance['revenue'].sum()

# Classify ABC
product_performance['ABC'] = pd.cut(
    product_performance['revenue_pct'].cumsum(),
    bins=[0, 0.8, 0.95, 1],
    labels=['A (Top 80%)', 'B (Next 15%)', 'C (Bottom 5%)']
)

print(product_performance.head())
```

---

### **Sample Output Insights**
#### **Customer ABC**:
| Customer ID | Total Spend | ABC Tier      |
|-------------|-------------|---------------|
| 123         | $12,450     | A (Top 20%)   |
| 456         | $890        | C (Bottom 50%)|

#### **Inventory ABC**:
| Product ID | Revenue Contribution | ABC Tier      |
|------------|-----------------------|---------------|
| 25         | 18%                  | A (Top 80%)   |
| 7          | 0.5%                 | C (Bottom 5%) |

---

### **Tools to Visualize**:
1. **Tableau/Power BI**: Create heatmaps of top-selling SKUs by store.  
2. **Python (Matplotlib/Seaborn)**: Plot customer CLV distribution.  
3. **SQL**: Run cohort analysis (e.g., repeat purchase rate).

This synthetic dataset will let you practice **inventory optimization**, **customer segmentation**, and **seasonal forecasting**â€”critical skills for data analysts in retail! ðŸ›’ðŸ“ˆ****
